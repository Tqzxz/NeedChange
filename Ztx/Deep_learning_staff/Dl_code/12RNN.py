
'''
    循环神经网络在语言模型中应用

    1. 语言模型的定义

    假设一个文本序列 = (w_1, w_2, w_3, ..., w_T), 这表示这段文本序列中有T个词.
    语言模型是一个概率模型， 目标是计算一段文本序列的联合概率 P(w_1, w_2, ..., w_T).

    2. 语言模型的计算
    一般地， 语言模型计算概率的定义公式是链式条件概率：
    P(w_1, w_2, ..., w_T) = P(w_1) * P(w_2 | w_1) * P(w_3 | w_1, w_2) * ... * P(w_T | w_1, w_2, ..., w_{T-1})

    为了简化链式条件概率的计算， 通常会进行马尔可夫链假设: 
            给定当前状态S_t, Future is independent to past state
            换句话说, 过去的影响已经造成了当前S_t状态的存在, 未来的状态只与当前状态有关.

    那么 P(w_3| w_1, w_2) 中， w_1 与 w_3无关, 就可以写成P(w_3 | w_2)

    简化后的语言模型计算公式为：
            P(w_1, w_2, ..., w_T) = P(w_T | w_{T-1}) : T = 1, 2, ..., T
        
    3. 问题
     (1). 语言模型的计算公式中， 需要计算每个词的条件概率 P(w_t | w_{t-1}),当T很大时, 计算量会非常大
     (2). 马尔可夫链假设的简化并不是完全的, 因为在实际应用中, 词与词之间的关系是复杂的, 可能需要考虑更长的上下文信息.
          当考虑不止前一个状态/Word时, 需要考虑更长的上下文信息, 这会导致计算量进一步增加. 

      e.g. 考虑一个2-gram模型, 需要计算 P(w_t | w_{t-1}, w_{t-2})
        P(w_5 | w_4, w_3) = P(w_5 | w_4, w_3) * P(w_4 | w_3, w_2) * P(w_3 | w_2, w_1) * P(w_2 | w_1) * p(w_1)

    4. 解决方案 RNN 



'''